<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Audio2Face-3D Browser SDK Demo</title>
    <style>
        * { box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            max-width: 1200px; margin: 0 auto; padding: 20px;
            background: #1a1a2e; color: #eee; line-height: 1.6;
        }
        h1 { color: #00d4ff; margin-bottom: 10px; }
        .subtitle { color: #888; margin-bottom: 30px; }
        .controls { background: #16213e; padding: 20px; border-radius: 8px; margin-bottom: 20px; border: 1px solid #0f3460; }
        h3 { margin-top: 0; color: #00d4ff; }
        button {
            background: #00d4ff; color: #1a1a2e; border: none; padding: 12px 24px;
            border-radius: 4px; cursor: pointer; font-size: 16px; margin: 5px; font-weight: 600; transition: all 0.2s;
        }
        button:hover:not(:disabled) { background: #00a8cc; transform: translateY(-1px); }
        button:disabled { background: #444; cursor: not-allowed; opacity: 0.6; }
        input[type="file"] { margin: 10px 0; color: #eee; }
        .status { padding: 12px; margin: 15px 0; border-radius: 4px; background: #0f3460; font-family: monospace; font-size: 14px; }
        .status.error { background: #5c1a1a; color: #ff6b6b; }
        .status.success { background: #1a5c1a; color: #6bff6b; }
        .status.loading { background: #5c4a1a; color: #ffd36b; }
        .blendshapes { display: grid; grid-template-columns: repeat(auto-fill, minmax(280px, 1fr)); gap: 12px; margin-top: 20px; max-height: 600px; overflow-y: auto; }
        .blendshape { background: #0f3460; padding: 12px; border-radius: 6px; display: flex; flex-direction: column; gap: 6px; }
        .blendshape-header { display: flex; justify-content: space-between; align-items: center; }
        .blendshape-name { font-size: 13px; color: #aaa; font-weight: 500; }
        .blendshape-value { font-family: monospace; font-size: 14px; color: #00d4ff; font-weight: 600; }
        .bar { width: 100%; height: 6px; background: #1a1a2e; border-radius: 3px; overflow: hidden; }
        .bar-fill { height: 100%; background: linear-gradient(90deg, #00d4ff, #00a8cc); border-radius: 3px; transition: width 0.1s ease-out; }
        #visualizer { width: 100%; height: 120px; background: #0f3460; border-radius: 4px; margin: 15px 0; }
        .stats { display: grid; grid-template-columns: repeat(auto-fit, minmax(150px, 1fr)); gap: 15px; margin: 15px 0; }
        .stat { background: #0f3460; padding: 15px; border-radius: 6px; text-align: center; }
        .stat-value { font-size: 24px; font-weight: bold; color: #00d4ff; }
        .stat-label { font-size: 12px; color: #888; margin-top: 5px; }
        .progress-bar { width: 100%; height: 8px; background: #0f3460; border-radius: 4px; overflow: hidden; margin: 10px 0; }
        .progress-fill { height: 100%; background: linear-gradient(90deg, #00d4ff, #00a8cc); border-radius: 4px; transition: width 0.3s ease; width: 0%; }
        .emotion-grid { display: grid; grid-template-columns: repeat(auto-fill, minmax(200px, 1fr)); gap: 10px; margin: 10px 0; }
        .emotion-item { display: flex; align-items: center; gap: 10px; }
        .emotion-item label { font-size: 13px; color: #aaa; min-width: 90px; text-transform: capitalize; }
        .emotion-item input[type="range"] { flex: 1; accent-color: #00d4ff; }
        .emotion-item .val { font-family: monospace; font-size: 13px; color: #00d4ff; min-width: 35px; }
    </style>
</head>
<body>
    <h1>Audio2Face-3D Browser SDK</h1>
    <p class="subtitle">Real-time audio-driven facial animation in the browser</p>

    <div class="controls">
        <h3>Model Status</h3>
        <div id="modelStatus" class="status loading">Initializing...</div>
        <div class="progress-bar"><div id="progressFill" class="progress-fill"></div></div>
        <div class="stats" id="modelStats" style="display: none;">
            <div class="stat"><div class="stat-value" id="backend">-</div><div class="stat-label">Backend</div></div>
            <div class="stat"><div class="stat-value" id="inputs">-</div><div class="stat-label">Inputs</div></div>
            <div class="stat"><div class="stat-value" id="outputs">-</div><div class="stat-label">Outputs</div></div>
            <div class="stat"><div class="stat-value" id="loadTime">-</div><div class="stat-label">Load Time</div></div>
        </div>
    </div>

    <div class="controls">
        <h3>Emotions</h3>
        <div id="emotionGrid" class="emotion-grid"></div>
    </div>

    <div class="controls">
        <h3>Audio Input</h3>
        <button id="startMic" disabled>Start Microphone (Realtime)</button>
        <button id="stopMic" disabled>Stop Microphone</button>
        <br><br>
        <input type="file" id="audioFile" accept="audio/*" disabled />
        <button id="processFile" disabled>Process Audio File</button>
        <canvas id="visualizer"></canvas>
        <div class="stats" id="audioStats" style="display: none;">
            <div class="stat"><div class="stat-value" id="fps">0</div><div class="stat-label">FPS</div></div>
            <div class="stat"><div class="stat-value" id="latency">0</div><div class="stat-label">Latency (ms)</div></div>
            <div class="stat"><div class="stat-value" id="chunks">0</div><div class="stat-label">Chunks Processed</div></div>
        </div>
    </div>

    <div class="controls">
        <h3>Blendshape Output</h3>
        <div id="outputStatus" class="status">Waiting for model to load...</div>
        <div id="blendshapes" class="blendshapes"></div>
    </div>

    <script src="onnxruntime-web.min.js"></script>
    <script type="module">
        import { Audio2FaceSDK } from './audio2face-sdk-browser.js';

        const sdk = new Audio2FaceSDK();
        let audioContext = null;
        let mediaStream = null;
        let isProcessing = false;
        let animationId = null;
        let chunkCount = 0;
        let totalChunks = 0;
        let lastFpsUpdate = performance.now();
        let fps = 0;

        const modelStatus = document.getElementById('modelStatus');
        const outputStatus = document.getElementById('outputStatus');
        const blendshapesContainer = document.getElementById('blendshapes');
        const visualizer = document.getElementById('visualizer');
        const canvasCtx = visualizer.getContext('2d');
        const progressFill = document.getElementById('progressFill');

        const emotionGrid = document.getElementById('emotionGrid');
        Audio2FaceSDK.EMOTIONS.forEach(name => {
            const item = document.createElement('div');
            item.className = 'emotion-item';
            const valSpan = document.createElement('span');
            valSpan.className = 'val';
            valSpan.textContent = '0.0';
            const slider = document.createElement('input');
            slider.type = 'range'; slider.min = '0'; slider.max = '1'; slider.step = '0.05'; slider.value = '0';
            slider.addEventListener('input', () => {
                const v = parseFloat(slider.value);
                valSpan.textContent = v.toFixed(1);
                sdk.setEmotion(name, v);
            });
            const label = document.createElement('label');
            label.textContent = name;
            item.append(label, slider, valSpan);
            emotionGrid.appendChild(item);
        });

        async function loadConfig() {
            try {
                const res = await fetch('a2f_ms_config.json');
                if (res.ok) { sdk.loadConfig(await res.json()); }
            } catch (e) {}
        }

        async function loadModelAutomatically() {
            await loadConfig();
            const possiblePaths = ['network_actual.onnx', 'network.onnx', 'model.onnx'];
            for (let i = 0; i < possiblePaths.length; i++) {
                const p = possiblePaths[i];
                try {
                    modelStatus.textContent = `Checking ${p}...`;
                    progressFill.style.width = `${((i + 0.5) / possiblePaths.length) * 50}%`;
                    const head = await fetch(p, { method: 'HEAD' });
                    if (!head.ok) continue;
                    const cl = head.headers.get('content-length');
                    const size = cl ? (parseInt(cl) / 1024 / 1024).toFixed(1) : 'unknown';
                    if (size !== 'unknown' && parseFloat(size) < 1) continue;
                    modelStatus.textContent = `Loading model (${size}MB)...`;
                    progressFill.style.width = '50%';
                    const startTime = performance.now();
                    const blob = await (await fetch(p)).blob();
                    progressFill.style.width = '75%';
                    modelStatus.textContent = 'Initializing inference session...';
                    await sdk.loadModel(blob, { useGPU: true });
                    const loadTime = Math.round(performance.now() - startTime);
                    const backend = sdk.actualBackend === 'webgpu' ? 'WebGPU' : 'WASM';
                    progressFill.style.width = '100%';
                    modelStatus.textContent = `Model loaded in ${loadTime}ms using ${backend}`;
                    modelStatus.className = 'status success';
                    document.getElementById('modelStats').style.display = 'grid';
                    document.getElementById('backend').textContent = backend;
                    document.getElementById('inputs').textContent = sdk.session.inputNames.length;
                    document.getElementById('outputs').textContent = sdk.session.outputNames.length;
                    document.getElementById('loadTime').textContent = loadTime + 'ms';
                    document.getElementById('startMic').disabled = false;
                    document.getElementById('audioFile').disabled = false;
                    document.getElementById('processFile').disabled = false;
                    outputStatus.textContent = 'Model ready! Start microphone or select audio file.';
                    outputStatus.className = 'status';
                    return;
                } catch (err) { console.warn(`Failed ${p}:`, err.message); }
            }
            modelStatus.textContent = 'Error: No valid model file found.';
            modelStatus.className = 'status error';
            progressFill.style.width = '100%';
            progressFill.style.background = '#ff6b6b';
        }

        window.addEventListener('load', loadModelAutomatically);

        document.getElementById('startMic').addEventListener('click', async () => {
            try {
                audioContext = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 16000 });
                mediaStream = await navigator.mediaDevices.getUserMedia({
                    audio: { sampleRate: 16000, channelCount: 1, echoCancellation: false, noiseSuppression: false }
                });
                const source = audioContext.createMediaStreamSource(mediaStream);
                const scriptNode = audioContext.createScriptProcessor(4096, 1, 1);
                scriptNode.onaudioprocess = (e) => {
                    if (isProcessing) return;
                    processAudioRealtime(e.inputBuffer.getChannelData(0));
                };
                source.connect(scriptNode);
                scriptNode.connect(audioContext.destination);
                document.getElementById('startMic').disabled = true;
                document.getElementById('stopMic').disabled = false;
                document.getElementById('audioStats').style.display = 'grid';
                outputStatus.textContent = 'Realtime processing active...';
                outputStatus.className = 'status success';
            } catch (err) {
                outputStatus.textContent = 'Microphone error: ' + err.message;
                outputStatus.className = 'status error';
            }
        });

        document.getElementById('stopMic').addEventListener('click', () => {
            if (mediaStream) mediaStream.getTracks().forEach(t => t.stop());
            if (audioContext) audioContext.close();
            if (animationId) cancelAnimationFrame(animationId);
            document.getElementById('startMic').disabled = false;
            document.getElementById('stopMic').disabled = true;
            document.getElementById('audioStats').style.display = 'none';
            outputStatus.textContent = 'Microphone stopped';
            outputStatus.className = 'status';
        });

        document.getElementById('processFile').addEventListener('click', async () => {
            const file = document.getElementById('audioFile').files[0];
            if (!file) { outputStatus.textContent = 'Please select an audio file'; outputStatus.className = 'status error'; return; }
            outputStatus.textContent = 'Processing...'; outputStatus.className = 'status loading';
            try {
                const t = performance.now();
                const result = await sdk.processAudioFile(file);
                displayResults(result);
                outputStatus.textContent = `Done! ${result.frameCount} frames in ${Math.round(performance.now() - t)}ms`;
                outputStatus.className = 'status success';
            } catch (err) { outputStatus.textContent = 'Error: ' + err.message; outputStatus.className = 'status error'; }
        });

        async function processAudioRealtime(audioData) {
            isProcessing = true;
            const t = performance.now();
            try {
                const result = await sdk.processAudioChunk(audioData);
                displayResults(result);
                chunkCount++; totalChunks++;
                const now = performance.now();
                if (now - lastFpsUpdate > 1000) { fps = chunkCount; chunkCount = 0; lastFpsUpdate = now; document.getElementById('fps').textContent = fps; }
                document.getElementById('latency').textContent = Math.round(now - t);
                document.getElementById('chunks').textContent = totalChunks;
            } catch (err) { console.error(err); }
            finally { isProcessing = false; }
        }

        function displayResults(result) {
            blendshapesContainer.innerHTML = '';
            if (result.blendshapes) {
                result.blendshapes.forEach(bs => {
                    const div = document.createElement('div');
                    div.className = 'blendshape';
                    div.innerHTML = `<div class="blendshape-header"><span class="blendshape-name">${bs.name}</span><span class="blendshape-value">${bs.value.toFixed(3)}</span></div><div class="bar"><div class="bar-fill" style="width: ${Math.min(100, bs.value * 100)}%"></div></div>`;
                    blendshapesContainer.appendChild(div);
                });
            }
        }

        function drawVisualizer() {
            animationId = requestAnimationFrame(drawVisualizer);
            const w = visualizer.width = visualizer.offsetWidth, h = visualizer.height = visualizer.offsetHeight;
            canvasCtx.fillStyle = '#0f3460'; canvasCtx.fillRect(0, 0, w, h);
            if (isProcessing) { canvasCtx.fillStyle = '#00d4ff'; canvasCtx.fillRect(0, h - 5, w, 5); }
        }
        drawVisualizer();
    </script>
</body>
</html>
